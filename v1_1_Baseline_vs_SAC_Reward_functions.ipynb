{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNmDDPRqKuTc6A3YdA/chqq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cosminconstantincostea/RL_cosmincostea/blob/Reinforcement-Learning/v1_1_Baseline_vs_SAC_Reward_functions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Intro\n",
        "the main goal fo this file is to chek the difference between the two learning apporaches of a SAC agent with StableBaselines3 library and comparing them with respect to a baseline.\n",
        "\n",
        "Environment used: CityLearn.\n",
        "Goal: Minimize electricty consumption.\n",
        "\n"
      ],
      "metadata": {
        "id": "AXct0wieOzY3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Common"
      ],
      "metadata": {
        "id": "4MGjL8SmPi3g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Libraries and General Parameters"
      ],
      "metadata": {
        "id": "3o_-PDdwPOWX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: update to install stable version from PyPi\n",
        "!pip install --no-deps CityLearn==2.2b0\n",
        "!pip install nrel-pysam\n",
        "\n",
        "# Prove standard RL algorithms\n",
        "!pip install stable-baselines3"
      ],
      "metadata": {
        "id": "_E_FwM3_Pns2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import the necessary pacakages\n",
        "\n",
        "# system operations\n",
        "import inspect\n",
        "import os\n",
        "import uuid\n",
        "import warnings\n",
        "\n",
        "# date and time\n",
        "import datetime\n",
        "\n",
        "# type hinting\n",
        "from typing import Any\n",
        "\n",
        "# User interaction\n",
        "from ipywidgets import Button, HTML\n",
        "from ipywidgets import Text, HBox, VBox\n",
        "\n",
        "# data visualization\n",
        "import matplotlib.colors as mcolors\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "\n",
        "# data manipulation\n",
        "from bs4 import BeautifulSoup\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import re\n",
        "import requests\n",
        "import simplejson as json\n",
        "\n",
        "# cityLearn\n",
        "from citylearn.agents.base import (\n",
        "    BaselineAgent,\n",
        "    Agent as RandomAgent\n",
        ")\n",
        "from citylearn.agents.rbc import HourRBC\n",
        "from citylearn.agents.q_learning import TabularQLearning\n",
        "from citylearn.citylearn import CityLearnEnv\n",
        "from citylearn.data import DataSet\n",
        "from citylearn.reward_function import RewardFunction\n",
        "from citylearn.wrappers import (\n",
        "    NormalizedObservationWrapper,\n",
        "    StableBaselines3Wrapper,\n",
        "    TabularQLearningWrapper,\n",
        "    DiscreteObservationWrapper,\n",
        "    DiscreteActionWrapper\n",
        ")\n",
        "\n",
        "# RL algorithms\n",
        "from stable_baselines3 import SAC\n",
        "from stable_baselines3 import DQN\n",
        "from stable_baselines3 import DDPG\n",
        "\n",
        "\n",
        "# Callbacks\n",
        "from stable_baselines3.common.callbacks import CheckpointCallback\n",
        "from stable_baselines3.common.callbacks import EvalCallback\n",
        "from stable_baselines3.common.callbacks import CallbackList\n",
        "from stable_baselines3.common.callbacks import StopTrainingOnRewardThreshold\n",
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "\n",
        "import time"
      ],
      "metadata": {
        "id": "01b_TecTPtrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Global settings for the remainder of the note book\n",
        "\n",
        "# All plotted figures without margins\n",
        "plt.rcParams['axes.ymargin'] = 0\n",
        "plt.rcParams['axes.xmargin'] = 0\n",
        "%matplotlib inline\n",
        "\n",
        "# ignore depractino warnings\n",
        "warnings.filterwarnings('ignore', category=DeprecationWarning)"
      ],
      "metadata": {
        "id": "s3fwGgEUPw_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Initialization\n",
        "\n",
        "Load the available data within the CityLean environment and display it."
      ],
      "metadata": {
        "id": "gpPTAukwPy6Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display('All CityLearn datasets:', sorted(DataSet.get_names()))\n",
        "DATASET_NAME = 'citylearn_challenge_2022_phase_1'\n",
        "\n",
        "schema = DataSet.get_schema(DATASET_NAME)\n",
        "root_directory = schema['root_directory']"
      ],
      "metadata": {
        "id": "8eA02Z-bQDFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Environment Description\n",
        "### Environment\n",
        "<figure class=\"image\">\n",
        "  <img src=\"https://github.com/intelligent-environments-lab/CityLearn/blob/master/assets/images/environment.jpg?raw=true\"  width=\"800\" alt=\"An overview of the heating, ventilation and air conditioning systems, energy storage systems, on-site electricity sources and grid interaction in buildings in the CityLearn environment.\" style=\"background-color:white;margin:20px;padding:5px\">\n",
        "  <figcaption>Figure: CityLearn building model including electricity sources that power controllable DERs including electric devices and ESSs, used to satisfy thermal and electrical loads as well as provide the grid with energy flexibility. A distinction is made between environment and control aspects of a building to show the transfer of actions from the control agent and reception of measurable observations by the control agent that quantifies the building's states (<a href=\"https://doi.org/10.48550/arXiv.2405.03848\">Nweye et al., 2024</a>).</figcaption>\n",
        "</figure>\n",
        "\n",
        "CityLearn models a district of buildings with similar or different loads, electric devices, energy storage systems (ESSs) and electricity sources that satisfy the loads as shown in the figure above. There is no upper limit on the number of buildings in a district and a district can have as few as one building.\n",
        "\n",
        "There are up to five loads ina building including:\n",
        "- space cooling;\n",
        "- space heating;\n",
        "- domestic hot water (DHW) - all the hot water that a home needs;\n",
        "- electric equipment - all the non-shiftable plug loads (lighting, entertainement, kitchen etc.);\n",
        "- electric (EV) loads-\n",
        "\n",
        "Building is modeled as single thermal zone where space thermal loads affect its indoor dry-bulb temperature. An occupant model has the ability to override the temperature setpoint.\n",
        "\n",
        "Not all loads need to exist in a building.\n",
        "Anyone or all of the loads are either known a priorii from building energy performance simulations (BEPS) or real-world measurements. In this instances the ideal load must be satisfied.\n",
        "Alternatively, they are controlled loads and are inferred at runtime e.g., heat pump power control drives space cooling or heating loads.\n",
        "\n",
        "To satisfy these loads in either the ideal or control-action case, CityLearn makes use of heating ventilation and air conditioning (HVAC) systems directly or ESSs through load shifting.\n",
        "- The `cooling_device`, `heating_device`, and `dhw_device` are HVAC electric device objects in CityLearn that are used to satisfy the space cooling, space heating and DHW heating loads respectively.\n",
        "- The `cooling_device` is a heat pump while the `heating_device` and `dhw_device` are either heat pump or electric heater type. These HVAC systems may be used to charge thermal energy storage (TES) systems in the building.\n",
        "\n",
        "\n",
        "There are up to five optional and controlled ESSs in a building including:\n",
        "TES Storages, DER type:\n",
        "- `cooling_storage`;\n",
        "- `heating_storage`;\n",
        "- `dhw_storage`;\n",
        "\n",
        "BESS - Battery energy storage system, DER type\n",
        "- `electrical_storage`\n",
        "- `electric_vehicle` - available on scheduel defined by its arrival and departure. The EV can be used in three modes:\n",
        "  - G2V: Grid-to-Vehicle;\n",
        "  - V2G: Vehicle-to-Grid;\n",
        "  - No control: EV acts as load without any possibile control over its charging;\n",
        "\n",
        "The electric devices are primarily powered by the electric grid. CityLearn at the time of writing, does not include a grid model so the power a building is able to draw from the grid at a given time step is unconstrained, except in the\n",
        "case of a power outage.\n",
        "\n",
        "Optionally, a building may have a photovoltaic (PV) system that provides self-generation as a first source of electricity before the grid. The optional `electrical_storage` and `electric_vehicle` are charged by the grid and PV but also augment the electricity supply when in discharge mode to supply the building with electricity. Excess self-generation, `electrical_storage`, and `electric_vehicle` discharge are sent to the grid as part of the building's net export.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hJ9QPX-hQKO3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Control Variables\n",
        "<table>\n",
        "    <tr>\n",
        "        <th>Name</th>\n",
        "        <th><code>a</code> range</th>\n",
        "        <th>Description</th>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td colspan=\"3\"><strong>Energy storage system</strong></td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td><code>cooling_storage</code></td>\n",
        "        <td>[-1, 1]</td>\n",
        "        <td>Proportion of <code>cooling_storage</code> capacity to be charged (<code>a</code> > 0) or discharged (<code>a</code> < 0).</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td><code>heating_storage</code></td>\n",
        "        <td>[-1, 1]</td>\n",
        "        <td>Proportion of <code>heating_storage</code> capacity to be charged (<code>a</code> > 0) or discharged (<code>a</code> < 0).</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td><code>dhw_storage</code></td>\n",
        "        <td>[-1, 1]</td>\n",
        "        <td>Proportion of <code>dhw_storage</code> capacity to be charged (<code>a</code> > 0) or discharged (<code>a</code> < 0).</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td><code>electrical_storage</code></td>\n",
        "        <td>[-1, 1]</td>\n",
        "        <td>Proportion of <code>electrical_storage</code> capacity to be charged (<code>a</code> > 0) or discharged (<code>a</code> < 0).</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td><code>electric_vehicle_storage</code></td>\n",
        "        <td>[-1, 1]</td>\n",
        "        <td>Proportion of <code>electric_vehicle_storage</code> capacity to be charged (<code>a</code> > 0) or discharged (<code>a</code> < 0).</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td colspan=\"3\"><strong>Electric device</strong></td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td><code>cooling_device</code></td>\n",
        "        <td>[0, 1]</td>\n",
        "        <td>Proportion of space <code>cooling_device</code> nominal power to be supplied.</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td><code>heating_device</code></td>\n",
        "        <td>[0, 1]</td>\n",
        "        <td>Proportion of space <code>heating_device</code> nominal power to be supplied.</td>\n",
        "    </tr>\n",
        "</table>\n",
        "\n",
        "The table above summarizes the continuous control action space in CityLearn where there are five ESS-related actions controlling the proportion of storage capacity to be charged or discharged and two HVAC electric device actions controlling the proportion of nominal power to be supplied. There are as many `electric_vehicle_storage` actions as there are EV chargers in a building.\n",
        "\n",
        "\n",
        "<figure class=\"image\">\n",
        "  <img src=\"https://github.com/intelligent-environments-lab/CityLearn/blob/master/assets/images/gymnasium_interface.jpg?raw=true\"  width=\"200\" alt=\"Farama Foundation Gymnasium interface.\" style=\"background-color:white;margin:20px;padding:5px\">\n",
        "  <figcaption>Figure: Farama Foundation Gymnasium interface (<a href=\"https://zenodo.org/records/10655021\">Towers et al., 2023</a>).</figcaption>\n",
        "</figure>\n",
        "\n",
        "The CityLearn environment makes use of the Farama Foundation Gymnasium interface for standardized RLC environment design, where there is an observation-action-reward exchange loop between the environment and control agent as the environment transitions from one time step to another. In the current time step, $t$, the control agent receives the environment's observations, $o_t$ and prescribes actions $a_t$. The actions are applied to the environment to affect the observations at the next time step, $o_{t + 1}$. $o_{t + 1}$ and a reward, $r_{t + 1}$ (from reward function, $R$) that quantifies the quality of $a_t$ in optimizing the outcome of a control objective or KPI are returned to the control agent to teach it to learn a control policy, $\\pi$. $\\pi$ maps actions to observations that maximize the\n",
        "cumulative reward over an episode i.e., the terminal state of the environment, after initialization ($t = 0$), beyond which there are new observations.\n",
        "\n",
        "<figure class=\"image\">\n",
        "  <img src=\"https://github.com/intelligent-environments-lab/CityLearn/blob/master/assets/images/control_architecture.jpg?raw=true\"  width=\"600\" alt=\"Single-agent (left), independent multi-agent (middle), and coordinated multi-agent (right) control configurations.\" style=\"background-color:white;margin:20px;padding:5px\">\n",
        "  <figcaption>Figure: Single-agent (left), independent multi-agent (middle), and coordinated multi-agent (right) control configurations (<a href=\"https://doi.org/10.48550/arXiv.2405.03848\">Nweye et al., 2024</a>).</figcaption>\n",
        "</figure>\n",
        "\n",
        "There are three possible control configurations in CityLearn namely:\n",
        "- **single-agent** - one-to-many relationship between the control agent and buildings where a centralized agent collects observations and prescribes actions for all DERs in the district and, receives a single reward value each time step to learn a generalized control policy. Akin to an energy aggregator.\n",
        "- **independent multi-agent** - configuration has a one-to-one agent-building relationship thus, there are as many rewards as buildings each time step and a unique control policy is learned for each building. The coordinated multi-agent configuration is similar to the independent multi-agent configuration except that agents can share information to achieve cooperative objectives e.g. district peak reduction or competitive objectives e.g. price bidding in the energy flexibility market.\n",
        "- **coordinated multi-agent**."
      ],
      "metadata": {
        "id": "G_Y1PeqqQW4o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preview of the data"
      ],
      "metadata": {
        "id": "oJBwTOrsU7mb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# change the suffix number in the next code line to a\n",
        "# number between 1 and 17 to preview other buildings\n",
        "building_name = 'Building_1'\n",
        "\n",
        "filename = schema['buildings'][building_name]['energy_simulation']\n",
        "filepath = os.path.join(root_directory, filename)\n",
        "building_data = pd.read_csv(filepath)\n",
        "display(building_data.head())"
      ],
      "metadata": {
        "id": "ZkN7HFV_VE92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(building_data)"
      ],
      "metadata": {
        "id": "AaZAgJc7bDrz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(1, 2, figsize=(14, 2.5))\n",
        "x = building_data.index\n",
        "y1 = building_data['non_shiftable_load']\n",
        "y2 = building_data['solar_generation']\n",
        "axs[0].plot(x, y1)\n",
        "axs[0].set_xlabel('Time step')\n",
        "axs[0].set_ylabel('Non-shiftable load\\n[kWh]')\n",
        "axs[1].plot(x, y2)\n",
        "axs[1].set_xlabel('Time step')\n",
        "axs[1].set_ylabel('Solar generation\\n[W/kW]')\n",
        "fig.suptitle(building_name)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "m-YA_JlrbGUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preview of the weather file\n"
      ],
      "metadata": {
        "id": "Osb_4H0KbMmX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filename = schema['buildings'][building_name]['weather']\n",
        "filepath = os.path.join(root_directory, filename)\n",
        "weather_data = pd.read_csv(filepath)\n",
        "display(weather_data.head())\n"
      ],
      "metadata": {
        "id": "D9U9zI6MbSLA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns = [\n",
        "    'outdoor_dry_bulb_temperature', 'outdoor_relative_humidity',\n",
        "    'diffuse_solar_irradiance', 'direct_solar_irradiance'\n",
        "]\n",
        "titles = [\n",
        "    'Outdoor dry-bulb\\ntemperature [C]', 'Relative humidity\\n[%]',\n",
        "    'Diffuse solar irradiance\\n[W/m2]', 'Direct solar irradiance\\n[W/m2]'\n",
        "]\n",
        "fig, axs = plt.subplots(2, 2, figsize=(14, 4.25))\n",
        "x = weather_data.index\n",
        "\n",
        "for ax, c, t in zip(fig.axes, columns, titles):\n",
        "    y = weather_data[c]\n",
        "    ax.plot(x, y)\n",
        "    ax.set_xlabel('Time step')\n",
        "    ax.set_ylabel(t)\n",
        "\n",
        "fig.align_ylabels()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tEOQsNbDbXYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preview of electricty price data"
      ],
      "metadata": {
        "id": "1NqnJpxHbZWB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filename = schema['buildings'][building_name]['pricing']\n",
        "filepath = os.path.join(root_directory, filename)\n",
        "pricing_data = pd.read_csv(filepath)\n",
        "display(pricing_data.head())"
      ],
      "metadata": {
        "id": "ZzJTvSYHbcmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random seed for reproducibility"
      ],
      "metadata": {
        "id": "K9dwe8V4QZG2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RANDOM_SEED=42"
      ],
      "metadata": {
        "id": "K6MJG_y6QcRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing\n",
        "\n",
        "Te main scope of this section to build functions for:\n",
        "- Bulding selection subset from the main set;\n",
        "- Simulation period selection from the main set;\n",
        "- Observation to use in the simulations;\n",
        "- Selection of the KPIs for the evaluation.\n"
      ],
      "metadata": {
        "id": "xjB6VCoVQxWS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Number of Building - Selection Function\n",
        "Scope: choose between the available buildings in the data set. Only the defined number of buildings will be used to improve training speed and model evaluation. Building 12 and Building 15 will be excluded by default."
      ],
      "metadata": {
        "id": "n3aF7irZRg1n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def select_buildings(\n",
        "dataset_name: str, count: int, seed: int, buildings_to_exclude: list[str] = None,\n",
        ") -> list[str]:\n",
        "    \"\"\" Randoms elect buildings from Citylearn dataset.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    dataset_name = str\n",
        "        CityLearn dataset to query buildings from.\n",
        "    count: int\n",
        "        Number of buildings to set as active in schema.\n",
        "    seed: int\n",
        "        Seed for pseudo-random number generator\n",
        "    building_to_exclude: list[str]\n",
        "        List of buildings to exclude from selection pool\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    buildings: list[str]\n",
        "    \"\"\"\n",
        "\n",
        "    assert 1 <= count <= 15, 'count must be between 1 and 15'\n",
        "\n",
        "    # set random seed\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # get all bulding names\n",
        "    schema = DataSet.get_schema(dataset_name)\n",
        "    buildings = list(schema['buildings'].keys())\n",
        "\n",
        "    # remove buildings 12 and 15 as they have peculiarities in thei data\n",
        "    # that are no relevant for this application\n",
        "    buildings_to_exclude = [] if buildings_to_exclude is None \\\n",
        "        else [b for b in buildings_to_exclude]\n",
        "    buildings_to_exclude += ['Building_12', 'Building_15']\n",
        "\n",
        "    for b in buildings_to_exclude:\n",
        "        if b in buildings:\n",
        "            buildings.remove(b)\n",
        "\n",
        "    # randomly select specified number of buildings\n",
        "    buildings = np.random.choice(buildings, count, replace=False)\n",
        "\n",
        "    return buildings.tolist()"
      ],
      "metadata": {
        "id": "g7i0GOiNR1XT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Simulation Period - Selection Function\n",
        "Scope:\n",
        "- Select a training period within the main dataset to train the agent;\n",
        "- Select a testing period within the main dataset to test the agent on;\n",
        "\n",
        "Currently the data is split in 70% training - 30% testing.\n",
        "\n"
      ],
      "metadata": {
        "id": "zyMkfluiR4nH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def select_simulation_period(\n",
        "    dataset_name: str, count: int, seed: int,\n",
        "    simulation_periods_to_exclude: list[tuple[int, int]] = None\n",
        ") -> tuple[int, int]:\n",
        "    \"\"\"Randomly select environment simulation start and end time steps\n",
        "    that cover a specified number of days.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    dataset_name: str\n",
        "        CityLearn dataset to query buildings from.\n",
        "    count: int\n",
        "        Number of simulation days.\n",
        "    seed: int\n",
        "        Seed for pseudo-random number generator.\n",
        "    simulation_periods_to_exclude: list[tuple[int, int]]\n",
        "        List of simulation periods to exclude from selection pool.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    simulation_start_time_step: int\n",
        "        The first time step in schema time series files to\n",
        "        be read when constructing the environment.\n",
        "    simulation_end_time_step: int\n",
        "        The last time step in schema time series files to\n",
        "        be read when constructing the environment.\n",
        "    \"\"\"\n",
        "\n",
        "    assert 1 <= count <= 365, 'count must be between 1 and 365.'\n",
        "\n",
        "    # set random seed\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # use any of the files to determine the total\n",
        "    # number of available time steps\n",
        "    schema = DataSet.get_schema(dataset_name)\n",
        "    filename = schema['buildings'][building_name]['carbon_intensity']\n",
        "    filepath = os.path.join(root_directory, filename)\n",
        "    time_steps = pd.read_csv(filepath).shape[0]\n",
        "\n",
        "    # set candidate simulation start time steps\n",
        "    # spaced by the number of specified days\n",
        "    simulation_start_time_step_list = np.arange(0, time_steps, 24*count)\n",
        "\n",
        "    # exclude period if needed\n",
        "    if simulation_periods_to_exclude is not None:\n",
        "        simulation_start_time_step_list_to_exclude = \\\n",
        "            [s for s, e in simulation_periods_to_exclude]\n",
        "        simulation_start_time_step_list = np.setdiff1d(\n",
        "            simulation_start_time_step_list,\n",
        "            simulation_start_time_step_list_to_exclude\n",
        "        )\n",
        "\n",
        "    else:\n",
        "        pass\n",
        "\n",
        "    # randomly select a simulation start time step\n",
        "    simulation_start_time_step = np.random.choice(\n",
        "        simulation_start_time_step_list, size=1\n",
        "    )[0]\n",
        "    simulation_end_time_step = simulation_start_time_step + 24*count - 1\n",
        "\n",
        "    # Split for testing and training\n",
        "    # 70% testing\n",
        "    # 30% training\n",
        "    training_days = int(count*0.7)\n",
        "    print(\"Training days:\", training_days)\n",
        "    testing_days = int(count*0.3)\n",
        "    print(\"Testing days:\", testing_days)\n",
        "\n",
        "    training_time_steps = training_days*24\n",
        "    print(\"Training time steps:\", training_time_steps)\n",
        "    testing_time_steps = testing_days*24\n",
        "    print(\"Testing time steps:\", testing_time_steps)\n",
        "\n",
        "    training_start_time_step = simulation_start_time_step\n",
        "    training_end_time_step   = simulation_start_time_step + training_time_steps -1\n",
        "\n",
        "    testing_start_time_step = training_end_time_step + 1\n",
        "    testing_end_time_step   = testing_start_time_step + testing_time_steps -1\n",
        "\n",
        "    return simulation_start_time_step, simulation_end_time_step,\\\n",
        "           training_start_time_step, training_end_time_step, \\\n",
        "           testing_start_time_step , testing_end_time_step\n",
        "\n"
      ],
      "metadata": {
        "id": "9mxKJpvVStWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building, Time Period, Observation Selection and Agent type definition\n",
        "\n",
        "\n",
        "In the following section the Number of buildings, the testing and training period and the observations used are selected.\n",
        "Also the type of the agent that will operate in the environment is selected to be:\n",
        "- Single agent;\n",
        "- Indipendent multi-agent;\n",
        "- Coordinated multi-agent."
      ],
      "metadata": {
        "id": "pkLog46VS9Om"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BUILDING_COUNT = 3\n",
        "BUILDINGS = select_buildings(\n",
        "    DATASET_NAME,\n",
        "    BUILDING_COUNT,\n",
        "    RANDOM_SEED,\n",
        ")\n",
        "print('Selected building:', BUILDINGS)\n",
        "\n",
        "DAY_COUNT = 30\n",
        "SIMULATION_START_TIME_STEP, SIMULATION_END_TIME_STEP,\\\n",
        "TRAINING_START_TIME_STEP, TRAINING_END_TIME_STEP,\\\n",
        "TESTING_START_TIME_STEP, TESTING_END_TIME_STEP = select_simulation_period(\n",
        "    DATASET_NAME,\n",
        "    DAY_COUNT,\n",
        "    RANDOM_SEED\n",
        ")\n",
        "\n",
        "EPISODE_TIME_STEPS = 24; # I choose to run it over 1 day for multiple periods\n",
        "\n",
        "print(\n",
        "    f'Selected {DAY_COUNT}-day simulation period:',\n",
        "    (SIMULATION_START_TIME_STEP, SIMULATION_END_TIME_STEP)\n",
        ")\n",
        "print(\n",
        "    f'Selected {DAY_COUNT*0.7}-day training period:',\n",
        "    (TRAINING_START_TIME_STEP, TRAINING_END_TIME_STEP)\n",
        ")\n",
        "print(f'Selected {DAY_COUNT*0.3}-day testing period:',\n",
        "    (TESTING_START_TIME_STEP, TESTING_END_TIME_STEP)\n",
        ")\n",
        "\n",
        "ACTIVE_OBSERVATIONS = ['hour']\n",
        "\n",
        "\n",
        "# The control will be centralized: one agent will control al the buildings.\n",
        "CENTRAL_AGENT = True"
      ],
      "metadata": {
        "id": "j6QxBlcrTPgy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Environment initialization check\n",
        "Check if the CityLearn Env initialize correctly"
      ],
      "metadata": {
        "id": "LPaGZEv7fjak"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Environment initialization\n",
        "env = CityLearnEnv(\n",
        "    DATASET_NAME,\n",
        "    central_agent = CENTRAL_AGENT,\n",
        "    buildings = BUILDINGS,\n",
        "    simulation_start_time_step = TRAINING_START_TIME_STEP,\n",
        "    simulation_end_time_step = TRAINING_END_TIME_STEP,\n",
        "    episode_time_steps = EPISODE_TIME_STEPS,\n",
        ")\n",
        "\n",
        "print('Current time step:', env.time_step)\n",
        "print('environment number of time steps:', env.time_steps)\n",
        "print('environment uses central agent:', env.central_agent)\n",
        "print('Number of buildings:', len(env.buildings))\n",
        "\n",
        "# Now let's calculate also the number of the episodes\n",
        "\n",
        "first_valid_start = TRAINING_START_TIME_STEP\n",
        "last_valid_start = TRAINING_END_TIME_STEP - EPISODE_TIME_STEPS\n",
        "\n",
        "print('Simulation_start_time_step', TRAINING_START_TIME_STEP)\n",
        "print('Simulation_end_time_step', TRAINING_END_TIME_STEP)\n",
        "print('Last episode starts:',last_valid_start)\n",
        "\n",
        "episodes = (last_valid_start - first_valid_start +1)/EPISODE_TIME_STEPS + 1\n",
        "print('Episodes available:',episodes)\n",
        "\n",
        "total_training_time_steps = TRAINING_END_TIME_STEP - TRAINING_START_TIME_STEP + 1\n",
        "print('Total time steps:', total_training_time_steps)\n",
        "\n",
        "episodes*EPISODE_TIME_STEPS"
      ],
      "metadata": {
        "id": "Kwl1i4elfrp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "observations, _ = env.reset()\n",
        "obs = observations[0]\n",
        "obs_names = env.observation_names[0]\n",
        "\n",
        "print(len(obs), len(obs_names))\n",
        "obs_dict = dict(zip(obs_names, obs))\n",
        "\n",
        "obs_dict,"
      ],
      "metadata": {
        "id": "4osC7hhyfunA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(obs_dict))"
      ],
      "metadata": {
        "id": "-eBMcxT7fwMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U64he4ypjgQ8"
      },
      "source": [
        "## Get KPIs for evaluations\n",
        "n.5 KPIs are considered to measure how good the agent is performing:\n",
        "1. electricity cost;\n",
        "2. carbon emissions;\n",
        "3. average daily peak;\n",
        "4. ramping;\n",
        "5. 1-load factor;\n",
        "\n",
        "3 and 5 are District level KPIs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convenience functions to display simulation results\n",
        "In the following some convenience function will be defined to plot the simulation results to confront different agents behaviour.\n"
      ],
      "metadata": {
        "id": "MkPcbRlbg63R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_kpis(env: CityLearnEnv) -> pd.DataFrame:\n",
        "    \"\"\"Returns evaluation KPIs.\n",
        "\n",
        "    Electricity cost and carbon emissions KPIs are provided\n",
        "    at the building-level and average district-level. Average daily peak,\n",
        "    ramping and (1 - load factor) KPIs are provided at the district level.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    env: CityLearnEnv\n",
        "        CityLearn environment instance.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    kpis: pd.DataFrame\n",
        "        KPI table.\n",
        "    \"\"\"\n",
        "    kpis = env.unwrapped.evaluate()\n",
        "\n",
        "    \"\"\"try:\n",
        "        kpis = env.unwrapped.evaluate()\n",
        "    except ZeroDivisionError:\n",
        "        # Skip or insert a dummy KPI table\n",
        "        kpis = pd.DataFrame(columns=['cost_function', 'value', 'level', 'name'])\n",
        "    \"\"\"\n",
        "\n",
        "    # names of KPIs to retrieve from evaluate function\n",
        "    kpi_names = {\n",
        "        'cost_total': 'Cost',\n",
        "        'carbon_emissions_total': 'Emissions',\n",
        "        'daily_peak_average': 'Avg. daily peak',\n",
        "        'ramping_average': 'Ramping',\n",
        "        'monthly_one_minus_load_factor_average': '1 - load factor'\n",
        "    }\n",
        "    kpis = kpis[\n",
        "        (kpis['cost_function'].isin(kpi_names))\n",
        "    ].dropna()\n",
        "    kpis['cost_function'] = kpis['cost_function'].map(lambda x: kpi_names[x])\n",
        "\n",
        "    # round up the values to 2 decimal places for readability\n",
        "    kpis['value'] = kpis['value'].round(2)\n",
        "\n",
        "    # rename the column that defines the KPIs\n",
        "    kpis = kpis.rename(columns={'cost_function': 'kpi'})\n",
        "\n",
        "    # Add epsilon to prevent division by zero in 1 - load factor calculation\n",
        "    epsilon = 1e-9\n",
        "    kpis.loc[kpis['kpi'] == '1 - load factor', 'value'] = kpis.loc[kpis['kpi'] == '1 - load factor', 'value'].apply(lambda x: x if x != 0 else epsilon)\n",
        "\n",
        "\n",
        "    return kpis"
      ],
      "metadata": {
        "id": "_Jn8smPshQC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Plot - Building KPI"
      ],
      "metadata": {
        "id": "J2kVlIqIhM6Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_building_kpis(envs: dict[str, CityLearnEnv]) -> plt.Figure:\n",
        "    \"\"\"Plots electricity consumption, cost and carbon emissions\n",
        "    at the building-level for different control agents in bar charts.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    envs: dict[str, CityLearnEnv]\n",
        "        Mapping of user-defined control agent names to environments\n",
        "        the agents have been used to control.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    fig: plt.Figure\n",
        "        Figure containing plotted axes.\n",
        "    \"\"\"\n",
        "\n",
        "    kpis_list = []\n",
        "\n",
        "    for k, v in envs.items():\n",
        "        kpis = get_kpis(v)\n",
        "        kpis = kpis[kpis['level']=='building'].copy()\n",
        "        kpis['building_id'] = kpis['name'].str.split('_', expand=True)[1]\n",
        "        kpis['building_id'] = kpis['building_id'].astype(int).astype(str)\n",
        "        kpis['env_id'] = k\n",
        "        kpis_list.append(kpis)\n",
        "\n",
        "    kpis = pd.concat(kpis_list, ignore_index=True, sort=False)\n",
        "    kpi_names= kpis['kpi'].unique()\n",
        "    column_count_limit = 3\n",
        "    row_count = math.ceil(len(kpi_names)/column_count_limit)\n",
        "    column_count = min(column_count_limit, len(kpi_names))\n",
        "    building_count = len(kpis['name'].unique())\n",
        "    env_count = len(envs)\n",
        "    figsize = (3.0*column_count, 0.3*env_count*building_count*row_count)\n",
        "    fig, _ = plt.subplots(\n",
        "        row_count, column_count, figsize=figsize, sharey=True\n",
        "    )\n",
        "\n",
        "    for i, (ax, (k, k_data)) in enumerate(zip(fig.axes, kpis.groupby('kpi'))):\n",
        "        sns.barplot(x='value', y='name', data=k_data, hue='env_id', ax=ax)\n",
        "        ax.set_xlabel(None)\n",
        "        ax.set_ylabel(None)\n",
        "        ax.set_title(k)\n",
        "\n",
        "        for j, _ in enumerate(envs):\n",
        "            ax.bar_label(ax.containers[j], fmt='%.2f')\n",
        "\n",
        "        if i == len(kpi_names) - 1:\n",
        "            ax.legend(\n",
        "                loc='upper left', bbox_to_anchor=(1.3, 1.0), framealpha=0.0\n",
        "            )\n",
        "        else:\n",
        "            ax.legend().set_visible(False)\n",
        "\n",
        "        for s in ['right','top']:\n",
        "            ax.spines[s].set_visible(False)\n",
        "\n",
        "    return fig"
      ],
      "metadata": {
        "id": "O7vubfRJi5Un"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Plot - Disctrict KPI"
      ],
      "metadata": {
        "id": "nLnMz8PKhR2A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_district_kpis(envs: dict[str, CityLearnEnv]) -> plt.Figure:\n",
        "    \"\"\"Plots electricity consumption, cost, carbon emissions,\n",
        "    average daily peak, ramping and (1 - load factor) at the\n",
        "    district-level for different control agents in a bar chart.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    envs: dict[str, CityLearnEnv]\n",
        "        Mapping of user-defined control agent names to environments\n",
        "        the agents have been used to control.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    fig: plt.Figure\n",
        "        Figure containing plotted axes.\n",
        "    \"\"\"\n",
        "\n",
        "    kpis_list = []\n",
        "\n",
        "    for k, v in envs.items():\n",
        "        kpis = get_kpis(v)\n",
        "        kpis = kpis[kpis['level']=='district'].copy()\n",
        "        kpis['env_id'] = k\n",
        "        kpis_list.append(kpis)\n",
        "\n",
        "    kpis = pd.concat(kpis_list, ignore_index=True, sort=False)\n",
        "    row_count = 1\n",
        "    column_count = 1\n",
        "    env_count = len(envs)\n",
        "    kpi_count = len(kpis['kpi'].unique())\n",
        "    figsize = (6.0*column_count, 0.225*env_count*kpi_count*row_count)\n",
        "    fig, ax = plt.subplots(row_count, column_count, figsize=figsize)\n",
        "    sns.barplot(x='value', y='kpi', data=kpis, hue='env_id', ax=ax)\n",
        "    ax.set_xlabel(None)\n",
        "    ax.set_ylabel(None)\n",
        "\n",
        "    for j, _ in enumerate(envs):\n",
        "        ax.bar_label(ax.containers[j], fmt='%.2f')\n",
        "\n",
        "    for s in ['right','top']:\n",
        "        ax.spines[s].set_visible(False)\n",
        "\n",
        "    ax.legend(loc='upper left', bbox_to_anchor=(1.3, 1.0), framealpha=0.0)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    return fig"
      ],
      "metadata": {
        "id": "IhtLBR6RhUv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Plot - Building Load Profile"
      ],
      "metadata": {
        "id": "uEDsCbkmhWuE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_building_load_profiles(\n",
        "    envs: dict[str, CityLearnEnv], daily_average: bool = None\n",
        ") -> plt.Figure:\n",
        "    \"\"\"Plots building-level net electricty consumption profile\n",
        "    for different control agents.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    envs: dict[str, CityLearnEnv]\n",
        "        Mapping of user-defined control agent names to environments\n",
        "        the agents have been used to control.\n",
        "    daily_average: bool, default: False\n",
        "        Whether to plot the daily average load profile.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    fig: plt.Figure\n",
        "        Figure containing plotted axes.\n",
        "    \"\"\"\n",
        "\n",
        "    daily_average = False if daily_average is None else daily_average\n",
        "    building_count = len(list(envs.values())[0].buildings)\n",
        "    column_count_limit = 4\n",
        "    row_count = math.ceil(building_count/column_count_limit)\n",
        "    column_count = min(column_count_limit, building_count)\n",
        "    figsize = (4.0*column_count, 1.75*row_count)\n",
        "    fig, _ = plt.subplots(row_count, column_count, figsize=figsize)\n",
        "\n",
        "    for i, ax in enumerate(fig.axes):\n",
        "        for k, v in envs.items():\n",
        "            y = v.unwrapped.buildings[i].net_electricity_consumption\n",
        "            y = np.reshape(y, (-1, 24)).mean(axis=0) if daily_average else y\n",
        "            x = range(len(y))\n",
        "            ax.plot(x, y, label=k)\n",
        "\n",
        "        ax.set_title(v.unwrapped.buildings[i].name)\n",
        "        ax.set_ylabel('kWh')\n",
        "\n",
        "        if daily_average:\n",
        "            ax.set_xlabel('Hour')\n",
        "            ax.xaxis.set_major_locator(ticker.MultipleLocator(2))\n",
        "\n",
        "        else:\n",
        "            ax.set_xlabel('Time step')\n",
        "            ax.xaxis.set_major_locator(ticker.MultipleLocator(24*7))\n",
        "\n",
        "        if i == building_count - 1:\n",
        "            ax.legend(\n",
        "                loc='upper left', bbox_to_anchor=(1.0, 1.0), framealpha=0.0\n",
        "            )\n",
        "        else:\n",
        "            ax.legend().set_visible(False)\n",
        "\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    return fig"
      ],
      "metadata": {
        "id": "_ET0U5o8hZ0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Plot - Distric Load Profile"
      ],
      "metadata": {
        "id": "_UqOVqRthbzH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_district_load_profiles(\n",
        "    envs: dict[str, CityLearnEnv], daily_average: bool = None\n",
        ") -> plt.Figure:\n",
        "    \"\"\"Plots district-level net electricty consumption profile\n",
        "    for different control agents.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    envs: dict[str, CityLearnEnv]\n",
        "        Mapping of user-defined control agent names to environments\n",
        "        the agents have been used to control.\n",
        "    daily_average: bool, default: False\n",
        "        Whether to plot the daily average load profile.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    fig: plt.Figure\n",
        "        Figure containing plotted axes.\n",
        "    \"\"\"\n",
        "\n",
        "    daily_average = False if daily_average is None else daily_average\n",
        "    figsize = (5.0, 1.5)\n",
        "    fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
        "\n",
        "    for k, v in envs.items():\n",
        "        y = v.unwrapped.net_electricity_consumption\n",
        "        y = np.reshape(y, (-1, 24)).mean(axis=0) if daily_average else y\n",
        "        x = range(len(y))\n",
        "        ax.plot(x, y, label=k)\n",
        "\n",
        "    ax.set_ylabel('kWh')\n",
        "\n",
        "    if daily_average:\n",
        "        ax.set_xlabel('Hour')\n",
        "        ax.xaxis.set_major_locator(ticker.MultipleLocator(2))\n",
        "\n",
        "    else:\n",
        "        ax.set_xlabel('Time step')\n",
        "        ax.xaxis.set_major_locator(ticker.MultipleLocator(24*7))\n",
        "\n",
        "    ax.legend(loc='upper left', bbox_to_anchor=(1.0, 1.0), framealpha=0.0)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    return fig"
      ],
      "metadata": {
        "id": "wjshFT4Chg-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Plot - SOC profiles\n"
      ],
      "metadata": {
        "id": "itJWJWQzhovd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_battery_soc_profiles(envs: dict[str, CityLearnEnv]) -> plt.Figure:\n",
        "    \"\"\"Plots building-level battery SoC profiles fro different control agents.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    envs: dict[str, CityLearnEnv]\n",
        "        Mapping of user-defined control agent names to environments\n",
        "        the agents have been used to control.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    fig: plt.Figure\n",
        "        Figure containing plotted axes.\n",
        "    \"\"\"\n",
        "\n",
        "    building_count = len(list(envs.values())[0].buildings)\n",
        "    column_count_limit = 4\n",
        "    row_count = math.ceil(building_count/column_count_limit)\n",
        "    column_count = min(column_count_limit, building_count)\n",
        "    figsize = (4.0*column_count, 1.75*row_count)\n",
        "    fig, _ = plt.subplots(row_count, column_count, figsize=figsize)\n",
        "\n",
        "    for i, ax in enumerate(fig.axes):\n",
        "        for k, v in envs.items():\n",
        "            y = np.array(v.unwrapped.buildings[i].electrical_storage.soc)\n",
        "            x = range(len(y))\n",
        "            ax.plot(x, y, label=k)\n",
        "\n",
        "        ax.set_title(v.unwrapped.buildings[i].name)\n",
        "        ax.set_xlabel('Time step')\n",
        "        ax.set_ylabel('SoC')\n",
        "        ax.xaxis.set_major_locator(ticker.MultipleLocator(24*7))\n",
        "        ax.set_ylim(0.0, 1.0)\n",
        "\n",
        "        if i == building_count - 1:\n",
        "            ax.legend(\n",
        "                loc='upper left', bbox_to_anchor=(1.0, 1.0), framealpha=0.0\n",
        "            )\n",
        "        else:\n",
        "            ax.legend().set_visible(False)\n",
        "\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    return fig"
      ],
      "metadata": {
        "id": "zLByFWHehr5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Plot - Simulation Summary"
      ],
      "metadata": {
        "id": "Nr0sPsmXht11"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_simulation_summary(envs: dict[str, CityLearnEnv]):\n",
        "    \"\"\"Plots KPIs, load and battery SoC profiles for different control agents.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    envs: dict[str, CityLearnEnv]\n",
        "        Mapping of user-defined control agent names to environments\n",
        "        the agents have been used to control.\n",
        "    \"\"\"\n",
        "\n",
        "    print('#'*8 + ' BUILDING-LEVEL ' + '#'*8)\n",
        "    print('Building-level KPIs:')\n",
        "    _ = plot_building_kpis(envs)\n",
        "    plt.show()\n",
        "\n",
        "    print('Building-level simulation period load profiles:')\n",
        "    _ = plot_building_load_profiles(envs)\n",
        "    plt.show()\n",
        "\n",
        "    print('Building-level daily-average load profiles:')\n",
        "    _ = plot_building_load_profiles(envs, daily_average=True)\n",
        "    plt.show()\n",
        "\n",
        "    print('Battery SoC profiles:')\n",
        "    _ = plot_battery_soc_profiles(envs)\n",
        "    plt.show()\n",
        "\n",
        "    print('#'*8 + ' DISTRICT-LEVEL ' + '#'*8)\n",
        "    print('District-level KPIs:')\n",
        "    _ = plot_district_kpis(envs)\n",
        "    plt.show()\n",
        "\n",
        "    print('District-level simulation period load profiles:')\n",
        "    _ = plot_district_load_profiles(envs)\n",
        "    plt.show()\n",
        "\n",
        "    print('District-level daily-average load profiles:')\n",
        "    _ = plot_district_load_profiles(envs, daily_average=True)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "y6wjdagOhyfs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Plot - Actions"
      ],
      "metadata": {
        "id": "rJZ_jeaEh0-q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_actions(actions_list: list[list[float]], building_names: list[str], title: str) -> plt.Figure:\n",
        "    \"\"\"Plots action time series for different buildings\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    actions_list: list[list[float]]\n",
        "        List of actions where each element with index, i,\n",
        "        in list is a list of the actions for different buildings\n",
        "        taken at time step i.\n",
        "    building_names: list[str]:\n",
        "        List of build names that map to the action lists.\n",
        "    title: str\n",
        "        Plot axes title\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    fig: plt.Figure\n",
        "        Figure with plotted axes\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(6, 1))\n",
        "    plot_data = pd.DataFrame(actions_list, columns=building_names)\n",
        "    x = list(range(plot_data.shape[0]))\n",
        "\n",
        "    for c in plot_data.columns:\n",
        "        y = plot_data[c].tolist()\n",
        "        ax.plot(x, y, label=c)\n",
        "\n",
        "    ax.legend(loc='upper left', bbox_to_anchor=(1.0, 1.0), framealpha=0.0)\n",
        "    ax.set_xlabel('Time step')\n",
        "    ax.set_ylabel(r'$\\frac{kWh}{kWh_{capacity}}$')\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(24*7))\n",
        "    ax.set_title(title)\n",
        "\n",
        "    return fig"
      ],
      "metadata": {
        "id": "kbtQx_bih4wc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Plot - Rewards"
      ],
      "metadata": {
        "id": "rsz2iUzYh6Gb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_rewards(ax: plt.Axes, rewards: list[float], title: str) -> plt.Axes:\n",
        "    \"\"\"Plots rewards over training episodes.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    rewards: list[float]\n",
        "        List of reward sum per episode.\n",
        "    title: str\n",
        "        Plot axes title\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    ax: plt.Axes\n",
        "        Plotted axes\n",
        "    \"\"\"\n",
        "\n",
        "    ax.plot(rewards)\n",
        "    ax.set_xlabel('Episode')\n",
        "    ax.set_ylabel('Reward')\n",
        "    ax.set_title(title)\n",
        "\n",
        "    return ax"
      ],
      "metadata": {
        "id": "2xhAce5uh7eH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baseline simulation\n",
        "In the following cells the baseline for the environment will be simulated and plotted."
      ],
      "metadata": {
        "id": "qs8wFsCMh-O3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Environment Creation\n"
      ],
      "metadata": {
        "id": "CdXUvTY3iHg_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_env = CityLearnEnv(\n",
        "    DATASET_NAME,\n",
        "    central_agent=CENTRAL_AGENT,\n",
        "    buildings=BUILDINGS,\n",
        "    active_observations=ACTIVE_OBSERVATIONS,\n",
        "    simulation_start_time_step=TESTING_START_TIME_STEP,\n",
        "    simulation_end_time_step=TESTING_END_TIME_STEP,\n",
        ")\n"
      ],
      "metadata": {
        "id": "OqokKGrpiNkg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Considerations about constructor argument `episode_time_step`\n",
        "`episode_time_step` control how long each episode is, from the documentation there are 3 options:\n",
        "\n",
        "For understanding we will consider:\n",
        "`simulation_start_time_step` = 0\n",
        "`simulation_end_time_step` = 720 = (30 days)*(24h)\n",
        "1. Set it as an `int`: e.g. `episode_time_step=24` means that each episode will last 24 steps, therefore my simulation will have 30 episodes.\n",
        "\n",
        "2. Set as a `list`: e.g. `[0,23],[24,47], ... , [697,729]` then:\n",
        "    * first episode runs 0 --> 23;\n",
        "    * second episode runs 24 --> 47.\n",
        "\n",
        "3. Not setting --> `Default`: will be automatically set:\n",
        "`simulation_end_time_step - simulation_start_time_step +1 `, and it will be set as **1 EPISODE**.\n",
        "\n",
        "Why was not set in the `baseline_env`?\n",
        "In the baseline env, agent won't do anything but read the data. Therefore it has to run only once over it.\n",
        "Same logic will be applied during the **Testing**, a new environment will be created with different simulation period and it will run over it only once.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Mm8kio1FGHaS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Agent Definition"
      ],
      "metadata": {
        "id": "I9Oqz1XmiaRd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_model = BaselineAgent(baseline_env)"
      ],
      "metadata": {
        "id": "u9VMDAl0iRh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "x6ef2ZobFKI0"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SzbpNpdqFN7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing"
      ],
      "metadata": {
        "id": "kni1xMaGFORF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# always start by reseting the environment\n",
        "observations, _ = baseline_env.reset()\n",
        "step_count = 0\n",
        "# step through the environment until terminal\n",
        "# state is reached i.e., the control episode ends\n",
        "while not baseline_env.terminated:   # usare gli step qui e non \"terminated\"\n",
        "    # select actions from the model\n",
        "    actions = baseline_model.predict(observations)\n",
        "    step_count = step_count + 1\n",
        "    # if step_count%20 == 0:\n",
        "      # print(actions)\n",
        "      # print(observations)\n",
        "\n",
        "    # apply selected actions to the environment\n",
        "    observations, _, _, _, _ = baseline_env.step(actions)\n",
        "\n",
        "# Note to self, the episode_count = episodes_time_steps - 1.\n",
        "# The env run the test the number of timesteps set in the env.\n",
        "# If you do not set the episode_timesteps, it will be automatically set to:\n",
        "# SIMULATION_START_TIME_STEPS - SIMULATION_END_TIME_STEPS + 1\n",
        "step_count"
      ],
      "metadata": {
        "id": "e7blG0qLFPqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot Results"
      ],
      "metadata": {
        "id": "5jrtsU3UiTCj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_simulation_summary({\n",
        "    'Baseline': baseline_env,\n",
        "})"
      ],
      "metadata": {
        "id": "_CndSXgxif4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SAC Simulation"
      ],
      "metadata": {
        "id": "zeFGrIfBFk0G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom Reward Definiton\n"
      ],
      "metadata": {
        "id": "2Jh3KT3YGotg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Base case reward\n",
        "The goal is to reduce the electricty price paid for the energy"
      ],
      "metadata": {
        "id": "yYAr5E9aGroA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomReward(RewardFunction):\n",
        "    def __init__(self, env_metadata: dict[str, Any]):\n",
        "        r\"\"\" Initialize CustomReward\n",
        "\n",
        "        Parameters\n",
        "        ------\n",
        "        env_metadata: dic[str, Any]:\n",
        "        General static information about the environment.\n",
        "        \"\"\"\n",
        "        super().__init__(env_metadata)\n",
        "\n",
        "    def calculate(\n",
        "        self, observations: list[dict[str,int | float]]\n",
        "    ) -> list[float]:\n",
        "        r\"\"\"Returns reward for most recent action.\n",
        "\n",
        "        The reward is designed to minimize electricity cost.\n",
        "        It is calculated for each building, i and summed to provide the agent\n",
        "        with a reward that is representative of all n buildings.\n",
        "        It encourages net-zero energy use by penalizing grid load satisfaction\n",
        "        when there is energy in the battery as well as penalizing\n",
        "        net export when the battery is not fully charged through the penalty\n",
        "        term. There is neither penalty nor reward when the battery\n",
        "        is fully charged during net export to the grid. Whereas, when the\n",
        "        battery is charged to capacity and there is net import from the\n",
        "        grid the penalty is maximized.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        observations: list[dict[str, int | float]]\n",
        "            List of all building observations at current\n",
        "            :py:attr:`citylearn.citylearn.CityLearnEnv.time_step`\n",
        "            that are got from calling\n",
        "            :py:meth:`citylearn.building.Building.observations`.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        reward: list[float]\n",
        "            Reward for transition to current timestep.\n",
        "        \"\"\"\n",
        "\n",
        "        reward_list = []\n",
        "\n",
        "        for o, m in zip(observations, self.env_metadata['buildings']):\n",
        "            cost = o['net_electricity_consumption']*o['electricity_pricing']\n",
        "            battery_soc = o['electrical_storage_soc']\n",
        "            penalty = -(1.0 + np.sign(cost)*battery_soc)\n",
        "            reward = penalty*abs(cost)\n",
        "            reward_list.append(reward)\n",
        "\n",
        "        reward = [sum(reward_list)]\n",
        "        '''\n",
        "        # Track worst reward seen so far\n",
        "        if not hasattr(self, \"min_reward\"):\n",
        "            self.min_reward = absolute_reward\n",
        "        self.min_reward = min(self.min_reward, absolute_reward)\n",
        "\n",
        "        # Normalize to [-1, 0]\n",
        "        relative_reward = absolute_reward / (abs(self.min_reward) + 1e-6)\n",
        "        '''\n",
        "        return reward"
      ],
      "metadata": {
        "id": "VKqD5tgUGyxq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1st attempt - Adaptive Weight Reward\n",
        "\n",
        "Mathematical Reward Definition\n",
        "Goal: single-step reward that minimize all the KPIs at once.\n",
        "\n",
        "Notation:\n",
        "- $B$ = number of buildings;\n",
        "- $E^b_h$ = net electricity consumption of building $b$ at hour $h$;\n",
        "- $T_h$ = electricty price at hour $h$;\n",
        "- $O_h$ = carbon intensity at hour $h$;\n",
        "- $E^{district}_h = \\Sigma^B_{b=1}E^b_h $ = total net electricty consumption of $B$ buildings.\n",
        "\n",
        "KPIs in one step (h):\n",
        "\n",
        "1. Cost contribution:\n",
        "\n",
        "\\begin{align}\n",
        "c_h = \\sum_{b=1}^B max (0, E^b_h) \\cdot T_h\n",
        "\\end{align}\n",
        "\n",
        "2. Carbon contribution:\n",
        "\n",
        "\\begin{align}\n",
        "co_h = \\sum_{b=1}^B max (0, E^b_h) \\cdot O_h\n",
        "\\end{align}\n",
        "\n",
        "3. Ramping contribution:\n",
        "\\begin{align}\n",
        "r_h = | E_h^{district} - E_{h-1}^{district}|\n",
        "\\end{align}\n",
        "\n",
        "4. Peak proxy (instantaneous load - penalizing high consumption\n",
        "\n",
        "\\begin{align}\n",
        "p_h = E_h^{district}\n",
        "\\end{align}\n",
        "\n",
        "5. Load factor (encouraging flat loads, penalize deviation from mean):\n",
        "\n",
        "\\begin{align}\n",
        "lf_h = \\frac{| E_h^{district} - E_{h-1}^{district}|}{\\bar{E}^{district}+\\epsilon}\n",
        "\\end{align}\n",
        "\n",
        "where:\n",
        " - $\\bar{E}^{district}$ is a moving average of recent district loads (e.g. 24h);\n",
        " - $\\epsilon$ is a coefficient to avoid dividing by zero.\n",
        "\n",
        "**Reward function definition**\n",
        "\n",
        "The goal is to minimize all KPIs by making the reward a negative weighted sum:\n",
        "\n",
        "\\begin{align}\n",
        "R_h = -\\left( \\alpha_c \\cdot \\hat{c}_h + \\alpha_{co} \\cdot \\hat{c}_o + \\alpha_p \\cdot \\hat{p}_h + \\alpha_r \\cdot \\hat{r}_h + \\alpha_f \\cdot  \\hat{lf_h} \\right)\n",
        "\\end{align}\n",
        "\n",
        "Where the $\\alpha_i$ are tunable weights (each coeff is between 0 and 1)\n",
        "and each term is normalized by dividing by the baseline:\n",
        "\n",
        "\\begin{align}\n",
        "\\hat{c}_h = \\frac{c_h}{c}, \\quad \\hat{c}_o = \\frac{c_{oh}}{c_{o}}, \\quad \\hat{p}_h = \\frac{p_h}{p}, \\quad \\hat{r}_h = \\frac{r_h}{r}, \\quad \\hat{l} f_h = l f_h\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "**Coefficent Update Strategy**\n",
        "1. Each KPI is normalized by its baseline;\n",
        "2. All $\\alpha_i$ coefficients are set equal to 1;\n",
        "3. All coeff have an adaptive weight update:\n",
        "\n",
        "\\begin{align}\n",
        "\\alpha_i \\leftarrow \\frac{1}{KPI_i + \\epsilon}\n",
        "\\end{align}\n",
        "\n",
        "so worse $KPI$ --> stronger the penalty during the training.\n"
      ],
      "metadata": {
        "id": "MAxnnSvxn2ix"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AdaptiveWeightReward(RewardFunction):\n",
        "    def __init__(self, env, epsilon=1e-6, log_interval=100):\n",
        "        super().__init__(env)\n",
        "        self.epsilon = epsilon\n",
        "        self.log_interval = log_interval\n",
        "        self.step_count = 0\n",
        "\n",
        "        self.prev_district_load = 0.0\n",
        "        self.district_load_hist = []\n",
        "\n",
        "        # Histories for adaptive weights\n",
        "        self.cost_hist = []\n",
        "        self.carbon_hist = []\n",
        "        self.peak_hist = []\n",
        "        self.ramp_hist = []\n",
        "        self.lf_hist = []\n",
        "\n",
        "        # Initialize alphas\n",
        "        self.alpha_c = 1.0\n",
        "        self.alpha_co = 1.0\n",
        "        self.alpha_p = 1.0\n",
        "        self.alpha_r = 1.0\n",
        "        self.alpha_lf = 1.0\n",
        "\n",
        "    def reward(self, reward):\n",
        "        obs = self.env.observe()\n",
        "\n",
        "        # Extract variables\n",
        "        prices = [o[\"electricity_pricing\"] for o in obs]\n",
        "        carbon = [o[\"carbon_intensity\"] for o in obs]\n",
        "        net_loads = [o[\"net_electricity_consumption\"] for o in obs]\n",
        "\n",
        "        district_load = np.sum(net_loads)\n",
        "\n",
        "        # Cost contribution\n",
        "        c_h = np.sum([max(0, net_loads[i]) * prices[i] for i in range(len(net_loads))])\n",
        "\n",
        "        # Carbon contribution\n",
        "        co_h = np.sum([max(0, net_loads[i]) * carbon[i] for i in range(len(net_loads))])\n",
        "\n",
        "        # Peak proxy\n",
        "        p_h = max(0, district_load)\n",
        "\n",
        "        # Ramping\n",
        "        r_h = abs(district_load - self.prev_district_load)\n",
        "\n",
        "        # Update state for next step\n",
        "        self.prev_district_load = district_load\n",
        "\n",
        "        # Load factor proxy (variance from mean)\n",
        "        self.district_load_hist.append(district_load)\n",
        "        if len(self.district_load_hist) > 24:\n",
        "            self.district_load_hist.pop(0)\n",
        "        mean_load = np.mean(self.district_load_hist) if self.district_load_hist else district_load\n",
        "        lf_h = abs(district_load - mean_load) / (mean_load + self.epsilon)\n",
        "\n",
        "        # Store histories\n",
        "        self.cost_hist.append(c_h)\n",
        "        self.carbon_hist.append(co_h)\n",
        "        self.peak_hist.append(p_h)\n",
        "        self.ramp_hist.append(r_h)\n",
        "        self.lf_hist.append(lf_h)\n",
        "\n",
        "        # Update adaptive weights (inverse of running mean)\n",
        "        self.alpha_c = 1.0 / (np.mean(self.cost_hist[-100:]) + self.epsilon)\n",
        "        self.alpha_co = 1.0 / (np.mean(self.carbon_hist[-100:]) + self.epsilon)\n",
        "        self.alpha_p = 1.0 / (np.mean(self.peak_hist[-100:]) + self.epsilon)\n",
        "        self.alpha_r = 1.0 / (np.mean(self.ramp_hist[-100:]) + self.epsilon)\n",
        "        self.alpha_lf = 1.0 / (np.mean(self.lf_hist[-100:]) + self.epsilon)\n",
        "\n",
        "        # Log adaptive weights every log_interval steps\n",
        "        self.step_count += 1\n",
        "        if self.step_count % self.log_interval == 0:\n",
        "            print(f\"Step {self.step_count}: alphas -> cost={self.alpha_c:.4f}, carbon={self.alpha_co:.4f}, \"\n",
        "                  f\"peak={self.alpha_p:.4f}, ramp={self.alpha_r:.4f}, load_factor={self.alpha_lf:.4f}\")\n",
        "\n",
        "        # Normalize values relative to running mean\n",
        "        norm_c = c_h / (np.mean([c_h, 1.0]) + self.epsilon)\n",
        "        norm_co = co_h / (np.mean([co_h, 1.0]) + self.epsilon)\n",
        "        norm_p = p_h / (np.mean([p_h, 1.0]) + self.epsilon)\n",
        "        norm_r = r_h / (np.mean([r_h, 1.0]) + self.epsilon)\n",
        "        norm_lf = lf_h\n",
        "\n",
        "        # Weighted negative sum with adaptive weights\n",
        "        custom_reward = - (\n",
        "            self.alpha_c * norm_c +\n",
        "            self.alpha_co * norm_co +\n",
        "            self.alpha_p * norm_p +\n",
        "            self.alpha_r * norm_r +\n",
        "            self.alpha_lf * norm_lf\n",
        "        )\n",
        "\n",
        "        '''\n",
        "        # Track worst reward seen so far\n",
        "        if not hasattr(self, \"min_reward\"):\n",
        "            self.min_reward = absolute_reward\n",
        "        self.min_reward = min(self.min_reward, absolute_reward)\n",
        "\n",
        "        # Normalize to [-1, 0]\n",
        "        relative_reward = absolute_reward / (abs(self.min_reward) + self.epsilon)\n",
        "        '''\n",
        "\n",
        "        return custom_reward\n"
      ],
      "metadata": {
        "id": "Wf_xmKd3onUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Environment Creation"
      ],
      "metadata": {
        "id": "N5TzIYBTFnU1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Environment with the base case reward funciton\n",
        "# br -> base reward\n",
        "sac_env_br= CityLearnEnv(\n",
        "    DATASET_NAME,\n",
        "    central_agent=CENTRAL_AGENT,\n",
        "    buildings=BUILDINGS,\n",
        "    active_observations=ACTIVE_OBSERVATIONS,\n",
        "    simulation_start_time_step=TRAINING_START_TIME_STEP,\n",
        "    simulation_end_time_step=TRAINING_END_TIME_STEP,\n",
        "    reward_function=CustomReward,\n",
        "    episode_time_steps=EPISODE_TIME_STEPS,\n",
        ")"
      ],
      "metadata": {
        "id": "MsPL3KU5FqIU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1st Wrap to categorize all the observations in the interval [0,1]\n",
        "sac_env_br = NormalizedObservationWrapper(sac_env_br)\n",
        "\n",
        "# 2nd Wrap to b eable to use the pre-installed algorithms within StableBaselines3\n",
        "sac_env_br = StableBaselines3Wrapper(sac_env_br)\n",
        "\n",
        "print(sac_env_br.reset())"
      ],
      "metadata": {
        "id": "G-vntidzFwEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Environment with the adaptive weight reward function\n",
        "# awr -> adapideweightReward\n",
        "sac_env_awr= CityLearnEnv(\n",
        "    DATASET_NAME,\n",
        "    central_agent=CENTRAL_AGENT,\n",
        "    buildings=BUILDINGS,\n",
        "    active_observations=ACTIVE_OBSERVATIONS,\n",
        "    simulation_start_time_step=TRAINING_START_TIME_STEP,\n",
        "    simulation_end_time_step=TRAINING_END_TIME_STEP,\n",
        "    reward_function=AdaptiveWeightReward,\n",
        "    episode_time_steps=EPISODE_TIME_STEPS,\n",
        ")\n",
        "\n",
        "# 1st Wrap to categorize all the observations in the interval [0,1]\n",
        "sac_env_awr = NormalizedObservationWrapper(sac_env_awr)\n",
        "\n",
        "# 2nd Wrap to b eable to use the pre-installed algorithms within StableBaselines3\n",
        "sac_env_awr = StableBaselines3Wrapper(sac_env_awr)\n",
        "\n",
        "print(sac_env_awr.reset())"
      ],
      "metadata": {
        "id": "KH6DZFPmo1X5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Agent definition"
      ],
      "metadata": {
        "id": "zByRqlyiF3ow"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Agent 1 definition - Base Case Reward\n",
        "This agent will be trained using `model.learn()` with the the `CustomReward` or base case reward function"
      ],
      "metadata": {
        "id": "pdpMVU69GKns"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inizialize the SAC MODEL\n",
        "sac_model_br = SAC(policy='MlpPolicy', env=sac_env_br, seed=RANDOM_SEED)"
      ],
      "metadata": {
        "id": "Z1WnpwFrF4wd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Agent 2 defintion - Adaptive Weight Reward\n",
        "This agent will be trained using `model.learn()` with the `AdapiveWeightReward` function."
      ],
      "metadata": {
        "id": "wiKGZWYHGHZG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Second model to compare training\n",
        "sac_model_awr = SAC(policy='MlpPolicy', env=sac_env_awr, seed=RANDOM_SEED)"
      ],
      "metadata": {
        "id": "PsSOLse7Ghtb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training\n",
        "\n",
        "### Consideration for the training on SB3.\n",
        "There are two ways available for training with SB3.\n",
        "\n",
        "1. `model.learn(total_timesteps = 30*24, reset_num_timesteps = False )` - In this case the total_timesteps are 720. If in the createiion of the citylearn environment the episode_timesteps are set to 24 the training will be done over the 30 episodes. The model won't run over the same 24 chunks as long as `reset_num_timesteps = False`.\n",
        "\n",
        "2. `for _ in range(30):\n",
        "        model.learn(total_timesteps=24, reset_num_timesteps=False)`\n",
        "\n",
        "Is the same as before, It just feel more manual and gives controlo to insert logic between episodes.\n",
        "\n",
        "\n",
        "**Some considerations**\n",
        "\n",
        "a. Option 1 is almost always better because everytime `model.learn()` is called DB3 has to:\n",
        "1. rebuild some training buffers;\n",
        "2. reset parts of the rollout manager;\n",
        "3. Re-run certain checks and initializations.\n",
        "\n",
        "So with the loop this has to happen 30x more often wasting time.\n",
        "\n",
        "b. SB3 algos store recent experiences in buffers (replay buffer for SAC, rollout buffer for PPO) across episodes.\n",
        "In the 1st option the replay buffers fill coninously for 720 steps, giving the agent a richer, more varied pool of experiences to learn from in each gradient update.\n",
        "\n",
        "In the 2nd option the buffer is only filled in tiny 24-steps burts, and in many cases, updates start from a much smaller set of experiences, which hurts tability\n",
        "\n",
        "c. Better gradient updates.\n",
        "For off-policy SAC algos like SA:\n",
        "- Replay buffer can sample from * multiple episode at once* if it is kept alive.\n",
        "- Calling `.learn()` in small chuncks still works (because `reset_num_timesteps = False`), but you're feeding the buffer in small trickles insted of a steady stream - less efficient for training.\n",
        "\n",
        "d. Logging and learning rate schedules\n",
        "SB3'S learning rate schedules and progrss meteres use the internal timestep counter.\n",
        "- Option 1, scheduler sees smooth 0 --> 720 progression.\n",
        "- Option 2, it still counts correctly if `reset_num_timesteps=Flase`, but logging and callbacks can trigger too frequently adding noise.\n"
      ],
      "metadata": {
        "id": "QzHdU6sVHN6c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Episode check and EPOCH definition\n",
        "This snap of code is meant to check the total episodes to run the algorithm and to define the `EPOCHS` for the second training method."
      ],
      "metadata": {
        "id": "9Z6vWzl6HPb5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sac_episodes = int(episodes)\n",
        "sac_episode_timesteps = sac_env_br.unwrapped.time_steps\n",
        "sac_total_timesteps = sac_episodes*sac_episode_timesteps\n",
        "\n",
        "print(sac_episodes)\n",
        "print(sac_episode_timesteps)\n",
        "print(sac_total_timesteps)\n",
        "\n",
        "# Ref without setting episode_time_steps = EPISODE_TIME_STEPS in env\n",
        "# sac episodes:           30\n",
        "# sac episodes timesteps: 720\n",
        "# sac total timesteps:    21600\n",
        "print(EPISODE_TIME_STEPS)\n",
        "\n",
        "EPOCHS = 50\n",
        "epochs = EPOCHS"
      ],
      "metadata": {
        "id": "vqRBZSFdHZSl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Callback function to monitor the training phase"
      ],
      "metadata": {
        "id": "GfStOto3HmXI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AverageRewardCallback(BaseCallback):\n",
        "    def __init__(self, verbose=1):\n",
        "        super().__init__(verbose)\n",
        "        self.episode_rewards = []   # Stores total reward per episode\n",
        "        self.relative_rewards = []\n",
        "        self.current_reward = 0\n",
        "        self.min_reward = None\n",
        "        self.epsilon = 1e-6\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        # Accumulate rewards for the current episode\n",
        "        self.current_reward += self.locals[\"rewards\"][0]\n",
        "\n",
        "        # When episode ends\n",
        "        if self.locals[\"dones\"][0]:\n",
        "          abs_reward = self.current_reward\n",
        "\n",
        "          # Track worst reward\n",
        "          if self.min_reward is None:\n",
        "            self.min_reward = abs_reward\n",
        "          self.min_reward = min(self.min_reward, abs_reward)\n",
        "\n",
        "          #Normalzie into [-1,0]\n",
        "          rel_reward = abs_reward / (abs(self.min_reward) + self.epsilon)\n",
        "\n",
        "          self.episode_rewards.append(abs_reward)\n",
        "          self.relative_rewards.append(rel_reward)\n",
        "\n",
        "          avg_rel = np.mean(self.relative_rewards)\n",
        "          avr_reward = np.mean(self.episode_rewards)\n",
        "\n",
        "          if self.verbose > 0:\n",
        "              print(f\"Episode {len(self.episode_rewards)} | \"\n",
        "                    f\"Abs Reward: {self.current_reward:.2f} | \"\n",
        "                    f\"Avg Reward: {avg_reward:.2f} |\"\n",
        "                    f\"Rel Reward: {avg_rel:2.f}\")\n",
        "\n",
        "          self.current_reward = 0  # reset for next episode\n",
        "\n",
        "        return True\n",
        "\n",
        "    def _on_training_end(self):\n",
        "        if self.verbose > 0:\n",
        "            print(f\"\\nTraining complete. Final Average Reward: {np.mean(self.episode_rewards):.2f}\")\n",
        "\n",
        "    def get_rewards(self):\n",
        "        \"\"\"Returns the list of episode rewards.\"\"\"\n",
        "        return self.episode_rewards\n",
        "    def get_relative_rewards(self):\n",
        "        \"\"\"Return the list of episode realive rewards\"\"\"\n",
        "        return self.relative_rewards\n"
      ],
      "metadata": {
        "id": "pTCncyR8Hq_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a function to plot the generic algo evaluation from the callback funcitons"
      ],
      "metadata": {
        "id": "2U85O9IuJmNL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### function to calculate the average reward per epoch\n",
        "def average_epoch_reward(episode_rewards, epochs, episodes):\n",
        "  average = []\n",
        "  for ii in range(0, len(episode_rewards), episodes):\n",
        "    chunk = episode_rewards[ii:ii+episodes]\n",
        "    if chunk:\n",
        "      average.append(sum(chunk)/len(chunk))\n",
        "    else:\n",
        "      average.append(0)\n",
        "  return average\n"
      ],
      "metadata": {
        "id": "GbRhtnRCHvwu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_aer(agent_rewards_dict, epochs, episodes_per_epoch):\n",
        "    \"\"\"\n",
        "    Plots the Average Epoch Rewards (AER) for multiple agents.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    agent_rewards_dict : dict\n",
        "        Dictionary where keys are agent names (str) and values are lists of episode rewards (from ARC.get_rewards()).\n",
        "        Example:\n",
        "            {\n",
        "                \"DQN\": rewards_dqn,\n",
        "                \"PPO\": rewards_ppo,\n",
        "                \"SAC\": rewards_sac\n",
        "            }\n",
        "\n",
        "    epochs : int\n",
        "        Total number of epochs to consider.\n",
        "\n",
        "    episodes_per_epoch : int\n",
        "        Number of episodes in each epoch.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    None\n",
        "        Displays a matplotlib plot of all agents' AER curves.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    for agent_name, episode_rewards in agent_rewards_dict.items():\n",
        "        aer_values = average_epoch_reward(episode_rewards, epochs, episodes_per_epoch)\n",
        "        plt.plot(range(1, len(aer_values) + 1), aer_values, label=agent_name)\n",
        "\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Average Reward per Epoch\")\n",
        "    plt.title(\"Average Epoch Rewards of Agents\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "VcxbU-ZYJt5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Callback to monitor the base care reward\n",
        "callback_1 = AverageRewardCallback(verbose=0)\n",
        "# Callback to mintor the adaptive weight reward\n",
        "callback_2 = AverageRewardCallback(verbose=0)"
      ],
      "metadata": {
        "id": "pDG1qySEH2gK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training Loop - Agent 1"
      ],
      "metadata": {
        "id": "M39B15tbIgO0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Theoretical Faster training\n",
        "sac_env_br.reset()\n",
        "start_time = time.perf_counter()\n",
        "sac_model_br.learn(\n",
        "      total_timesteps=sac_total_timesteps*epochs,\n",
        "      reset_num_timesteps = False,\n",
        "      callback=callback_1,\n",
        "       progress_bar=True\n",
        "      )\n",
        "elapsed_big = time.perf_counter() - start_time\n",
        "print(\"Agent 1 - Training time: \", elapsed_big)\n"
      ],
      "metadata": {
        "id": "JnRhwrv3IjyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Traning Loop - Agent 2"
      ],
      "metadata": {
        "id": "5RYqTS31ry50"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Theoretical Faster training\n",
        "sac_env_awr.reset()\n",
        "start_time = time.perf_counter()\n",
        "sac_model_awr.learn(\n",
        "      total_timesteps=sac_total_timesteps*epochs,\n",
        "      reset_num_timesteps = False,\n",
        "      callback=callback_2,\n",
        "       progress_bar=True\n",
        "      )\n",
        "elapsed_big = time.perf_counter() - start_time\n",
        "print(\"Agent 2 - Training time: \", elapsed_big)\n"
      ],
      "metadata": {
        "id": "L6Ypj0pFr2fY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_aer(\n",
        "\n",
        "    {\n",
        "        \"Agent 1\": callback_1.get_relative_rewards(),\n",
        "        \"Agent 2\": callback_2.get_relative_rewards(),\n",
        "    },\n",
        "    epochs = epochs,\n",
        "    episodes_per_epoch = sac_episodes\n",
        ")\n",
        "\n",
        "plot_aer(\n",
        "        {\n",
        "        \"Agent 1\": callback_1.get_rewards(),\n",
        "        \"Agent 2\": callback_2.get_rewards(),\n",
        "    },\n",
        "    epochs = epochs,\n",
        "    episodes_per_epoch = sac_episodes\n",
        ")\n",
        ""
      ],
      "metadata": {
        "id": "2X3Dy0R6ZI8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing\n",
        "In this section the algo will be tested.\n"
      ],
      "metadata": {
        "id": "DqxbDCxma-qA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Testing environment creation"
      ],
      "metadata": {
        "id": "ZMnFUMNkbDsU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creation of the Testing Environment\n",
        "\n",
        "sac_env_test_br = CityLearnEnv(\n",
        "    DATASET_NAME,\n",
        "    central_agent=CENTRAL_AGENT,\n",
        "    buildings=BUILDINGS,\n",
        "    active_observations=ACTIVE_OBSERVATIONS,\n",
        "    simulation_start_time_step=TESTING_START_TIME_STEP,\n",
        "    simulation_end_time_step=TESTING_END_TIME_STEP,\n",
        "    reward_function=CustomReward,\n",
        "    #episode_time_steps=EPISODE_TIME_STEPS,\n",
        ")\n",
        "# 1st Wrap to categorize all the observations in the interval [0,1]\n",
        "sac_env_test_br = NormalizedObservationWrapper(sac_env_test_br)\n",
        "# 2nd Wrap to b eable to use the pre-installed algorithms within StableBaselines3\n",
        "sac_env_test_br = StableBaselines3Wrapper(sac_env_test_br)"
      ],
      "metadata": {
        "id": "OItYp69JbITP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Yn5yh7k4-2q8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creation of the Testing Environment\n",
        "\n",
        "sac_env_test_awr = CityLearnEnv(\n",
        "    DATASET_NAME,\n",
        "    central_agent=CENTRAL_AGENT,\n",
        "    buildings=BUILDINGS,\n",
        "    active_observations=ACTIVE_OBSERVATIONS,\n",
        "    simulation_start_time_step=TESTING_START_TIME_STEP,\n",
        "    simulation_end_time_step=TESTING_END_TIME_STEP,\n",
        "    reward_function=AdaptiveWeightReward,\n",
        "    #episode_time_steps=EPISODE_TIME_STEPS,\n",
        ")\n",
        "# 1st Wrap to categorize all the observations in the interval [0,1]\n",
        "sac_env_test_awr = NormalizedObservationWrapper(sac_env_test_awr)\n",
        "# 2nd Wrap to b eable to use the pre-installed algorithms within StableBaselines3\n",
        "sac_env_test_awr = StableBaselines3Wrapper(sac_env_test_awr)"
      ],
      "metadata": {
        "id": "6GOHzhKf-Vdq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "env_list = [sac_env_test_br,sac_env_test_awr,]\n",
        "model_list = [sac_model_br, sac_model_awr]"
      ],
      "metadata": {
        "id": "BaTuh8JxuwzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(len(env_list))"
      ],
      "metadata": {
        "id": "KHTntcxsvG3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Evaluation"
      ],
      "metadata": {
        "id": "TBrQwX_BbSAs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "verbose = 0\n",
        "sac_action_matrix = []\n",
        "reward_matrix = []\n",
        "counter_step_vector = []\n",
        "\n",
        "for env in range(len(env_list)):\n",
        "  observations, _ = env_list[env].reset()\n",
        "  #print(observations)\n",
        "  sac_action_list = []\n",
        "  reward_list = []\n",
        "  counter_steps = 0\n",
        "\n",
        "  while not env_list[env].unwrapped.terminated:\n",
        "    actions, _ = model_list[env].predict(observations,\n",
        "                                   deterministic=True)\n",
        "\n",
        "    observations, reward,_,_,_ = env_list[env].step(actions)\n",
        "\n",
        "    sac_action_list.append(actions)\n",
        "    reward_list.append(reward)\n",
        "    counter_steps = counter_steps + 1\n",
        "\n",
        "    if counter_steps%50==0 and verbose==1:\n",
        "      print(actions)\n",
        "      print(observations)\n",
        "\n",
        "  counter_step_vector.append(counter_steps)\n",
        "\n",
        "  print(\"Agent:\", model_list[env])\n",
        "  print(\"Env:\", env_list[env])\n",
        "  sac_action_matrix.append(sac_action_list)\n",
        "  reward_matrix.append(reward_list)\n",
        "\n",
        "plot_simulation_summary({\n",
        "    'Agent 1': sac_env_test_br.unwrapped,\n",
        "    'Agent 2': sac_env_test_awr.unwrapped,\n",
        "})\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Pkx70JAIbYAf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}